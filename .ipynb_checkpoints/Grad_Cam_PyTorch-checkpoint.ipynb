{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec085f0c-b887-4cfa-8a8b-ee52c6f9b968",
   "metadata": {},
   "source": [
    "# Utilisation du Grad_CAM pour un réseau VGG19\n",
    "___\n",
    "\n",
    "Ce code est inspiré de plusieurs articles et livres pour implémenter un Grad_CAM sur un réseau VGG19 avec des images issus de la banque d'image ImageNet. Comme certaines fonctionnalités ne sont pas présentes dans PyTorch, nous allons faire cette implémentation en suivant plusieurs étapes :\n",
    "- Charger le modèle VGG19\n",
    "- Trouver sa dernière couche convolutive\n",
    "- Calculer la classe la plus probable\n",
    "- Prenez le gradient du logit de classe par rapport aux cartes d’activation que nous venons d’obtenir\n",
    "- Regrouper les dégradés\n",
    "- Pondérer les canaux de la carte par les gradients groupés correspondants\n",
    "- Interpoler la carte thermique\n",
    "\n",
    "## 1ère étape : charger le modèle VGG19\n",
    "\n",
    "Ici, j’importe tous les éléments standard que nous utilisons pour travailler avec les réseaux de neurones dans PyTorch. J’utilise la transformation de base nécessaire pour utiliser n’importe quel modèle formé sur le jeu de données ImageNet, y compris la normalisation de l’image. Je vais nourrir une image à la fois, c’est pourquoi je définis mon ensemble de données comme étant l’image des éléphants, dans le but d’obtenir des résultats similaires à ceux de l'article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15f97f0-12d1-43a6-8d3b-5d0566771edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchvision.models import vgg19\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# use the ImageNet transformation\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# define a 1 image dataset\n",
    "dataset = datasets.ImageFolder(root='./data/Elephants', transform=transform)\n",
    "\n",
    "# define the dataloader to load that single image\n",
    "dataloader = data.DataLoader(dataset=dataset, shuffle=False, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdfea0b-3612-4484-a2d3-64f47880ba17",
   "metadata": {},
   "source": [
    "## 2eme étape : trouver la dernière couche convolutive\n",
    "\n",
    "Il y a un instrument de rappel dans PyTorch : les crochets. \n",
    "Les crochets peuvent être utilisés dans différents scénarios, le nôtre est l’un d’entre eux. La documentation nous dit :\n",
    "\n",
    "___\"Le crochet sera appelé chaque fois qu’un gradient par rapport au tenseur est calculé.\"___\n",
    "\n",
    "Nous savons maintenant que nous devons enregistrer le crochet arrière sur la carte d’activation de la dernière couche convolutive de notre modèle VGG19. Trouvons où accrocher. \n",
    "On peut facilement observer l’architecture VGG19 en appelant : _vgg19(pretrained=True)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640673f2-eb34-44bc-9ca1-24eddcb84248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a09c14-1072-4fb7-a5b7-6414938f2556",
   "metadata": {},
   "source": [
    "Dans l’image, nous voyons toute l’architecture VGG19. La dernière couche convolutive est donc ___(34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))___ (avec sa fonction d'activation ___(35): ReLU(inplace=True)___). Eh bien, nous savons maintenant que nous voulons enregistrer le crochet arrière à la 35ème couche du bloc de fonctionnalités de notre réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae9ba8e-eae5-4b57-aeab-eee784a85caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        # get the pretrained VGG19 network\n",
    "        self.vgg = vgg19(pretrained=True)\n",
    "        \n",
    "        # disect the network to access its last convolutional layer\n",
    "        self.features_conv = self.vgg.features[:36]\n",
    "        \n",
    "        # get the max pool of the features stem\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        \n",
    "        # get the classifier of the vgg19\n",
    "        self.classifier = self.vgg.classifier\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "    \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features_conv(x)\n",
    "        \n",
    "        # register the hook\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        # apply the remaining pooling\n",
    "        x = self.max_pool(x)\n",
    "        x = x.view((1, -1))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136704b1-6b82-4630-94dd-f4a82e6bf8cc",
   "metadata": {},
   "source": [
    "## 3ème étape : calculer sa classe la plus probable\n",
    "\n",
    "Tout d’abord, faisons passer le foward pass à travers le réseau avec l’image des éléphants et voyons ce que le VGG19 prédit. N’oubliez pas de mettre votre modèle en mode d’évaluation, sinon vous pouvez obtenir des résultats très aléatoires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9108a44f-39e5-4bde-9b3b-0a725229f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the VGG model\n",
    "vgg = VGG()\n",
    "\n",
    "# set the evaluation mode\n",
    "vgg.eval()\n",
    "\n",
    "# get the image from the dataloader\n",
    "img, _ = next(iter(dataloader))\n",
    "\n",
    "# get the most likely prediction of the model\n",
    "pred = vgg(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5be69a-b840-4ea4-bf45-5f4cc22d2f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([386])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax(dim=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa2d721-ef7d-405e-a330-8c1477f8cabb",
   "metadata": {},
   "source": [
    "## 4ème étape : prendre le gradient de logit de la classe probable et interpoler la carte graphique\n",
    "\n",
    "Maintenant, nous allons faire la rétro-propagation avec le logit de la 386ème classe qui représente le 'African_elephant' dans le jeu de données ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a847d91b-605a-43de-9c41-24b4aa138b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25cdb3eeca0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the gradient of the output with respect to the parameters of the model\n",
    "pred[:, 386].backward()\n",
    "\n",
    "# pull the gradients out of the model\n",
    "gradients = vgg.get_activations_gradient()\n",
    "\n",
    "# pool the gradients across the channels\n",
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "# get the activations of the last convolutional layer\n",
    "activations = vgg.get_activations(img).detach()\n",
    "\n",
    "# weight the channels by corresponding gradients\n",
    "for i in range(512):\n",
    "    activations[:, i, :, :] *= pooled_gradients[i]\n",
    "    \n",
    "# average the channels of the activations\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "# relu on top of the heatmap\n",
    "# expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "\n",
    "# draw the heatmap\n",
    "plt.matshow(heatmap.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253276a5-996e-494b-ad81-2623925121cf",
   "metadata": {},
   "source": [
    "Enfin, nous obtenons la carte thermique pour l’image de l’éléphant. Il s’agit d’une image monocanal 14x14. __La taille est dictée par les dimensions spatiales des cartes d’activation dans la dernière couche convolutive du réseau.__\n",
    "\n",
    "Maintenant, nous pouvons utiliser OpenCV pour interpoler la carte thermique et la projeter sur l’image originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa6751a-42b3-426c-92fa-485c4bbc98ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('./data/Elephants/data/elephants.jpg')\n",
    "\n",
    "heatmap = np.array(heatmap)\n",
    "\n",
    "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "superimposed_img = heatmap * 0.4 + img\n",
    "\n",
    "cv2.imwrite('./map.jpg', superimposed_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a9741d-3b71-444e-b0a0-fa174b4aee13",
   "metadata": {},
   "source": [
    "Dans l’image ci-dessous, nous pouvons voir les zones de l’image que notre réseau VGG19 a prises le plus au sérieux pour décider quelle classe (« African_elephant ») attribuer à l’image. Nous pouvons supposer que le réseau a pris la forme de la tête et des oreilles des éléphants comme un signe fort de la présence d’un éléphant dans l’image.\n",
    "\n",
    "Nous pouvons tester avec d'autres images, tels que des chats, des chiens, des requins, etc... Nous pouvons aussi tester d'autres modèles de CNN et voir la différence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
